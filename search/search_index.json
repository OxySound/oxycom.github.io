{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to OxySound \u00b6 Introduction \u00b6 OxySound sends data seamlessly over sound waves to enhance end-user experiences and add value to existing hardware. It uses any existing speaker or microphone, and is configurable to use audible or inaudible near-ultrasonic frequencies. OxySound is designed for simplicity, eliminating connectivity headaches and simplifying everyday tasks like connecting to Wi-Fi networks, sharing contact details, and making peer-to-peer payments. Why ? \u00b6 You can find speakers in most places, and that makes this technology cheap and can be used in cases where communication is restricted.","title":"Introduction"},{"location":"#welcome-to-oxysound","text":"","title":"Welcome to OxySound"},{"location":"#introduction","text":"OxySound sends data seamlessly over sound waves to enhance end-user experiences and add value to existing hardware. It uses any existing speaker or microphone, and is configurable to use audible or inaudible near-ultrasonic frequencies. OxySound is designed for simplicity, eliminating connectivity headaches and simplifying everyday tasks like connecting to Wi-Fi networks, sharing contact details, and making peer-to-peer payments.","title":"Introduction"},{"location":"#why","text":"You can find speakers in most places, and that makes this technology cheap and can be used in cases where communication is restricted.","title":"Why ?"},{"location":"how-it-works/","text":"How it works \u00b6 Overview \u00b6 OxySound enables your apps to send and receive information using sound. We encode your data as an audio signal, which can be transmitted by any device with a speaker and received by any device with a microphone with the OxySound SDK. It is designed to be robust over distances of several metres, in noisy, everyday environments. As the transmission takes place entirely via audio signals, no internet connection or prior pairing is required, and any device within hearing range can receive the data. Audio can be generated on-device from a dynamic data payload, or recorded as an audio file for later playback with or without a compainion audio track.","title":"How it works"},{"location":"how-it-works/#how-it-works","text":"","title":"How it works"},{"location":"how-it-works/#overview","text":"OxySound enables your apps to send and receive information using sound. We encode your data as an audio signal, which can be transmitted by any device with a speaker and received by any device with a microphone with the OxySound SDK. It is designed to be robust over distances of several metres, in noisy, everyday environments. As the transmission takes place entirely via audio signals, no internet connection or prior pairing is required, and any device within hearing range can receive the data. Audio can be generated on-device from a dynamic data payload, or recorded as an audio file for later playback with or without a compainion audio track.","title":"Overview"},{"location":"components/sdk-android/","text":"Getting Started - Android \u00b6 AAR \u00b6 Copy the oxy.aar file to your app/libs directory. Add the following to the dependencies block of your Module build.gradle Gradle script. To instruct Gradle where to find the local .aar file, add flatDir section to the repositories block. (You\u2019ll need to add a repositories block if one does not already exist). repositories { flatDir { dirs 'libs' } } Permissions \u00b6 Declare your app's audio permissions Add the following to your AndroidManifest.xml, inside the bottom of the element. <uses-permission android:name=\"android.permission.MODIFY_AUDIO_SETTINGS\" /> <uses-permission android:name=\"android.permission.RECORD_AUDIO\" /> <uses-permission android:name=\"android.permission.INTERNET\" /> <uses-permission android:name=\"android.permission.ACCESS_NETWORK_STATE\" /> SDK requires at minimum of Android 5.0.x which is Android API level 26. Import the SDK \u00b6 import com.oxy.AndroidOxyCore.OxyCore import com.oxy.AndroidOxyCore.OxyCoreEvent Instantiate the SDK private lateinit var SDKOxyCore: OxyCore class MainActivity : AppCompatActivity(), OxyCoreEvent { override fun onCreate(savedInstanceState: Bundle?) { super.onCreate(savedInstanceState) setContentView(R.layout.activity_main) //Provide Context for Callback SDKOxyCore = OxyCore(this) } } //Callback override fun IdWith(Id: String) { } Request microphone permissions and start the engine override fun onResume() { super.onResume() SDKOxyCore.Listen() } States \u00b6 Pause \u00b6 override fun onPause() { super.onPause() SDKOxyCore.Stop() } Destory the instance \u00b6 Stop and close the SDK when activity is destroyed. In order to make sure the SDK will be closed, we need to call the close method to empty the memory and to delete the instance when the activity is destroyed. override fun onDestroy() { super.onDestroy() SDKOxyCore.Stop() } You are now ready to start using OxySound in your own application.","title":"Android"},{"location":"components/sdk-android/#getting-started-android","text":"","title":"Getting Started - Android"},{"location":"components/sdk-android/#aar","text":"Copy the oxy.aar file to your app/libs directory. Add the following to the dependencies block of your Module build.gradle Gradle script. To instruct Gradle where to find the local .aar file, add flatDir section to the repositories block. (You\u2019ll need to add a repositories block if one does not already exist). repositories { flatDir { dirs 'libs' } }","title":"AAR"},{"location":"components/sdk-android/#permissions","text":"Declare your app's audio permissions Add the following to your AndroidManifest.xml, inside the bottom of the element. <uses-permission android:name=\"android.permission.MODIFY_AUDIO_SETTINGS\" /> <uses-permission android:name=\"android.permission.RECORD_AUDIO\" /> <uses-permission android:name=\"android.permission.INTERNET\" /> <uses-permission android:name=\"android.permission.ACCESS_NETWORK_STATE\" /> SDK requires at minimum of Android 5.0.x which is Android API level 26.","title":"Permissions"},{"location":"components/sdk-android/#import-the-sdk","text":"import com.oxy.AndroidOxyCore.OxyCore import com.oxy.AndroidOxyCore.OxyCoreEvent Instantiate the SDK private lateinit var SDKOxyCore: OxyCore class MainActivity : AppCompatActivity(), OxyCoreEvent { override fun onCreate(savedInstanceState: Bundle?) { super.onCreate(savedInstanceState) setContentView(R.layout.activity_main) //Provide Context for Callback SDKOxyCore = OxyCore(this) } } //Callback override fun IdWith(Id: String) { } Request microphone permissions and start the engine override fun onResume() { super.onResume() SDKOxyCore.Listen() }","title":"Import the SDK"},{"location":"components/sdk-android/#states","text":"","title":"States"},{"location":"components/sdk-android/#pause","text":"override fun onPause() { super.onPause() SDKOxyCore.Stop() }","title":"Pause"},{"location":"components/sdk-android/#destory-the-instance","text":"Stop and close the SDK when activity is destroyed. In order to make sure the SDK will be closed, we need to call the close method to empty the memory and to delete the instance when the activity is destroyed. override fun onDestroy() { super.onDestroy() SDKOxyCore.Stop() } You are now ready to start using OxySound in your own application.","title":"Destory the instance"},{"location":"components/sdk-arm/","text":"Getting Started - ARM \u00b6 The OxySound SDK for Arm Cortex processors allows you to send and receive data-over-sound from within your embedded C/C++ application. It harnesses the powerful in-built FPU and optimised CMSIS-DSP library to enable on-chip encoding and decoding of OxySound signals. Requirements \u00b6 To build an embedded system that uses the OxySound Arm SDK, you will need the following components. Arm Cortex-M4 or Arm Cortex-M7 CPU Audio ADC/DAC capable of at least 16kHz operation For input, a microphone. We recommend a MEMS mic, ideally with a flat frequency response. For output, a loudspeaker.","title":"Arm / Mbed"},{"location":"components/sdk-arm/#getting-started-arm","text":"The OxySound SDK for Arm Cortex processors allows you to send and receive data-over-sound from within your embedded C/C++ application. It harnesses the powerful in-built FPU and optimised CMSIS-DSP library to enable on-chip encoding and decoding of OxySound signals.","title":"Getting Started - ARM"},{"location":"components/sdk-arm/#requirements","text":"To build an embedded system that uses the OxySound Arm SDK, you will need the following components. Arm Cortex-M4 or Arm Cortex-M7 CPU Audio ADC/DAC capable of at least 16kHz operation For input, a microphone. We recommend a MEMS mic, ideally with a flat frequency response. For output, a loudspeaker.","title":"Requirements"},{"location":"components/sdk-iphone/","text":"Getting Started - iOS \u00b6 Framework \u00b6 Import the SDK into your project by dragging and dropping the .framework file into your Xcode project. Set \u2018Copy items if needed\u2019 if the framework is not already within your project folder. Add the framework to Linked Frameworks and Libraries Go to the Project Settings, under the General tab, and add Oxy.framework to \"Linked Frameworks and Libraries\". Add a microphone privacy statement \u00b6 This text is displayed when your app first launches and asks the user for microphone permission, and should describe why you are requesting access to the microphone. The request message should be short and specific and include an example of how the microphone data will be used, for example We need access to the microphone to receive messages from nearby devices using sound. Under the Info tab in the Project Settings, add a Custom iOS Target Property called \"Privacy - Microphone Usage Description\". Disable Bitcode for the SDK \u00b6 Under the Build Settings tab in the Project Settings, under Build Options, set \"Enable Bitcode\" to No. Add a bridging header to the framework [Swift Only] Add a new Objective-C file to your Swift project. Xcode will prompt if you want to configure your project with a bridging header. Press yes. Once the bridging header is added to your project, you can delete the Objective-C file if you want to. Inside the bridging header, import the Oxy framework. This will make it available throughout your project\u2019s Swift files. Now for the code... \u00b6 Include the SDK and its associated configuration header: import Oxy And instantiate the SDK (A key will required in the future) let OxyManager = Oxy.instance() oxyManager?.delegate=self \u00b6 To receive this data on a second device, simply implement the delegate. class ViewController: UIViewController, oxyDelegate { } func oxyId(with oxy_id: String!) { DispatchQueue.main.async { //Do some magic } } Start oxyManager!.listen() You are now ready to start using OxySound in your own application. Advanced usage \u00b6 Preselected frequencies and tone seperations func custom(freq: Float, sep: Int) { oxyManager!.setCustomBaseFreq(freq, withSeparation: Int32(sep)) oxyManager!.configure(oxyManager, with: CUSTOM) self.oxyManager!.listen() } To limit user interaction with broadcasts a set distance can be set and when below this distance the SDK will respond. if(self.oxyManager!.distanceVol() < self.amax ) { } Send audio from the SDK. Type options are 0/1 or 2 this relates to should the broadcast be mixed with an external media file or not Broadcast tone with SDK configuration func sendAudio(){ checkDeviceVolumeLevel() oxyManager!.play(\"829450000\", withType: 0) } Audible R2D2 tone func sendAudio(){ checkDeviceVolumeLevel() oxyManager!.play(\"829450000\", withType: 1) } External media let soundURLg = Bundle.main.url(forResource: \"storm\", withExtension: \"wav\") ... oxyManager!.play(\"929450000\", withType: 2) SPAlert.present(title: \"On device audio mixed with message\", message: \"929450000\", image: UIImage(imageLiteralResourceName: \"stormz\"))","title":"iPhone"},{"location":"components/sdk-iphone/#getting-started-ios","text":"","title":"Getting Started - iOS"},{"location":"components/sdk-iphone/#framework","text":"Import the SDK into your project by dragging and dropping the .framework file into your Xcode project. Set \u2018Copy items if needed\u2019 if the framework is not already within your project folder. Add the framework to Linked Frameworks and Libraries Go to the Project Settings, under the General tab, and add Oxy.framework to \"Linked Frameworks and Libraries\".","title":"Framework"},{"location":"components/sdk-iphone/#add-a-microphone-privacy-statement","text":"This text is displayed when your app first launches and asks the user for microphone permission, and should describe why you are requesting access to the microphone. The request message should be short and specific and include an example of how the microphone data will be used, for example We need access to the microphone to receive messages from nearby devices using sound. Under the Info tab in the Project Settings, add a Custom iOS Target Property called \"Privacy - Microphone Usage Description\".","title":"Add a microphone privacy statement"},{"location":"components/sdk-iphone/#disable-bitcode-for-the-sdk","text":"Under the Build Settings tab in the Project Settings, under Build Options, set \"Enable Bitcode\" to No. Add a bridging header to the framework [Swift Only] Add a new Objective-C file to your Swift project. Xcode will prompt if you want to configure your project with a bridging header. Press yes. Once the bridging header is added to your project, you can delete the Objective-C file if you want to. Inside the bridging header, import the Oxy framework. This will make it available throughout your project\u2019s Swift files.","title":"Disable Bitcode for the SDK"},{"location":"components/sdk-iphone/#now-for-the-code","text":"Include the SDK and its associated configuration header: import Oxy And instantiate the SDK (A key will required in the future) let OxyManager = Oxy.instance() oxyManager?.delegate=self","title":"Now for the code..."},{"location":"components/sdk-iphone/#_1","text":"To receive this data on a second device, simply implement the delegate. class ViewController: UIViewController, oxyDelegate { } func oxyId(with oxy_id: String!) { DispatchQueue.main.async { //Do some magic } } Start oxyManager!.listen() You are now ready to start using OxySound in your own application.","title":""},{"location":"components/sdk-iphone/#advanced-usage","text":"Preselected frequencies and tone seperations func custom(freq: Float, sep: Int) { oxyManager!.setCustomBaseFreq(freq, withSeparation: Int32(sep)) oxyManager!.configure(oxyManager, with: CUSTOM) self.oxyManager!.listen() } To limit user interaction with broadcasts a set distance can be set and when below this distance the SDK will respond. if(self.oxyManager!.distanceVol() < self.amax ) { } Send audio from the SDK. Type options are 0/1 or 2 this relates to should the broadcast be mixed with an external media file or not Broadcast tone with SDK configuration func sendAudio(){ checkDeviceVolumeLevel() oxyManager!.play(\"829450000\", withType: 0) } Audible R2D2 tone func sendAudio(){ checkDeviceVolumeLevel() oxyManager!.play(\"829450000\", withType: 1) } External media let soundURLg = Bundle.main.url(forResource: \"storm\", withExtension: \"wav\") ... oxyManager!.play(\"929450000\", withType: 2) SPAlert.present(title: \"On device audio mixed with message\", message: \"929450000\", image: UIImage(imageLiteralResourceName: \"stormz\"))","title":"Advanced usage"},{"location":"components/sdk-linux/","text":"Getting Started - PI/Linux \u00b6 The OxySound SDK for Arm Cortex processors allows you to send and receive data-over-sound from within your embedded C/C++ application for encoding and decoding of OxySound signals. Requirements \u00b6 To build an embedded system that uses the OxySound Pi SDK, you will need the following components. Arm Cortex-A72 architecture variant CPU For input, a microphone. We recommend the ReSpeaker 2 mics Hat. For output, a loudspeaker.","title":"Pi/Linux"},{"location":"components/sdk-linux/#getting-started-pilinux","text":"The OxySound SDK for Arm Cortex processors allows you to send and receive data-over-sound from within your embedded C/C++ application for encoding and decoding of OxySound signals.","title":"Getting Started - PI/Linux"},{"location":"components/sdk-linux/#requirements","text":"To build an embedded system that uses the OxySound Pi SDK, you will need the following components. Arm Cortex-A72 architecture variant CPU For input, a microphone. We recommend the ReSpeaker 2 mics Hat. For output, a loudspeaker.","title":"Requirements"},{"location":"components/sdk-mac/","text":"Getting Started - MacOS \u00b6 Framework \u00b6 Import the SDK into your project by dragging and dropping the .framework file into your Xcode project. Set \u2018Copy items if needed\u2019 if the framework is not already within your project folder. Add the framework to Linked Frameworks and Libraries Go to the Project Settings, under the General tab, and add Oxy.framework to \"Linked Frameworks and Libraries\". Add a microphone privacy statement \u00b6 This text is displayed when your app first launches and asks the user for microphone permission, and should describe why you are requesting access to the microphone. The request message should be short and specific and include an example of how the microphone data will be used, for example We need access to the microphone to receive messages from nearby devices using sound. Under the Info tab in the Project Settings, add a Custom iOS Target Property called \"Privacy - Microphone Usage Description\". Disable Bitcode for the SDK \u00b6 Under the Build Settings tab in the Project Settings, under Build Options, set \"Enable Bitcode\" to No. Add a bridging header to the framework [Swift Only] Add a new Objective-C file to your Swift project. Xcode will prompt if you want to configure your project with a bridging header. Press yes. Once the bridging header is added to your project, you can delete the Objective-C file if you want to. Inside the bridging header, import the Oxy framework. This will make it available throughout your project\u2019s Swift files. Now for the code... \u00b6 Include the SDK and its associated configuration header: import Oxy And instantiate the SDK (A key will required in the future) let OxyManager = Oxy.instance() oxyManager?.delegate=self \u00b6 To receive this data on a second device, simply implement the delegate. class ViewController: UIViewController, oxyDelegate { } func oxyId(with oxy_id: String!) { DispatchQueue.main.async { //Do some magic } } Start oxyManager!.listen() You are now ready to start using OxySound in your own application.","title":"MacOS"},{"location":"components/sdk-mac/#getting-started-macos","text":"","title":"Getting Started - MacOS"},{"location":"components/sdk-mac/#framework","text":"Import the SDK into your project by dragging and dropping the .framework file into your Xcode project. Set \u2018Copy items if needed\u2019 if the framework is not already within your project folder. Add the framework to Linked Frameworks and Libraries Go to the Project Settings, under the General tab, and add Oxy.framework to \"Linked Frameworks and Libraries\".","title":"Framework"},{"location":"components/sdk-mac/#add-a-microphone-privacy-statement","text":"This text is displayed when your app first launches and asks the user for microphone permission, and should describe why you are requesting access to the microphone. The request message should be short and specific and include an example of how the microphone data will be used, for example We need access to the microphone to receive messages from nearby devices using sound. Under the Info tab in the Project Settings, add a Custom iOS Target Property called \"Privacy - Microphone Usage Description\".","title":"Add a microphone privacy statement"},{"location":"components/sdk-mac/#disable-bitcode-for-the-sdk","text":"Under the Build Settings tab in the Project Settings, under Build Options, set \"Enable Bitcode\" to No. Add a bridging header to the framework [Swift Only] Add a new Objective-C file to your Swift project. Xcode will prompt if you want to configure your project with a bridging header. Press yes. Once the bridging header is added to your project, you can delete the Objective-C file if you want to. Inside the bridging header, import the Oxy framework. This will make it available throughout your project\u2019s Swift files.","title":"Disable Bitcode for the SDK"},{"location":"components/sdk-mac/#now-for-the-code","text":"Include the SDK and its associated configuration header: import Oxy And instantiate the SDK (A key will required in the future) let OxyManager = Oxy.instance() oxyManager?.delegate=self","title":"Now for the code..."},{"location":"components/sdk-mac/#_1","text":"To receive this data on a second device, simply implement the delegate. class ViewController: UIViewController, oxyDelegate { } func oxyId(with oxy_id: String!) { DispatchQueue.main.async { //Do some magic } } Start oxyManager!.listen() You are now ready to start using OxySound in your own application.","title":""},{"location":"components/sdk-wasm/","text":"Getting Started - WebAssembly \u00b6 The OxySound WebAssembly SDK brings our technology to the web, allowing you to send and receive data over sound in the browser, using a simple JavaScript interface. With the WebAssembly SDK you can integrate OxySound into web pages and apps, and the exact same code can be executed on many different devices, both desktop and mobile. The SDK never sends audio data to the cloud, running all of the audio processing locally on your device. Known Issues \u00b6 1). Android and iOS devices do not detect frequencies above 8kHz and 11kHz respectively in the browser. However ultrasonic protocols are an option for desktop only applications. 2). The SDK will not work on iOS Chrome, as getUserMedia is not supported in a WKWebView. See the open radar bug report.","title":"WebAssembly"},{"location":"components/sdk-wasm/#getting-started-webassembly","text":"The OxySound WebAssembly SDK brings our technology to the web, allowing you to send and receive data over sound in the browser, using a simple JavaScript interface. With the WebAssembly SDK you can integrate OxySound into web pages and apps, and the exact same code can be executed on many different devices, both desktop and mobile. The SDK never sends audio data to the cloud, running all of the audio processing locally on your device.","title":"Getting Started - WebAssembly"},{"location":"components/sdk-wasm/#known-issues","text":"1). Android and iOS devices do not detect frequencies above 8kHz and 11kHz respectively in the browser. However ultrasonic protocols are an option for desktop only applications. 2). The SDK will not work on iOS Chrome, as getUserMedia is not supported in a WKWebView. See the open radar bug report.","title":"Known Issues"},{"location":"project/","text":"Sample audio \u00b6 Mixed with audio file \u00b6 Youtube / Netflix intergration \u00b6","title":"Overview"},{"location":"project/#sample-audio","text":"","title":"Sample audio"},{"location":"project/#mixed-with-audio-file","text":"","title":"Mixed with audio file"},{"location":"project/#youtube-netflix-intergration","text":"","title":"Youtube / Netflix intergration"},{"location":"tutorials/","text":"Tutorials \u00b6 OxySound SDKs make sending data with audio incredibly straightforward for developers. Data is provided to the SDKs as an array of bytes. This means that any arbitrary data can be sent in its existing form without the SDK requiring any complicated conversions of data types or esoteric schema. Transmission of the data can be via audible or inaudible ultrasonic audio depending on the configuration of your OxySound SDK. And you can get started with both the ultrasound and standard protocols for free in minutes.","title":"Tutorials"},{"location":"tutorials/#tutorials","text":"OxySound SDKs make sending data with audio incredibly straightforward for developers. Data is provided to the SDKs as an array of bytes. This means that any arbitrary data can be sent in its existing form without the SDK requiring any complicated conversions of data types or esoteric schema. Transmission of the data can be via audible or inaudible ultrasonic audio depending on the configuration of your OxySound SDK. And you can get started with both the ultrasound and standard protocols for free in minutes.","title":"Tutorials"},{"location":"tutorials/android-hello/","text":"iPhone App \u00b6 package com.example.oxysound import android.Manifest import android.content.pm.PackageManager import android.graphics.Color import android.os.Bundle import android.util.Log import android.widget.Toast import androidx.appcompat.app.AppCompatActivity import androidx.core.app.ActivityCompat import com.oxy.AndroidOxyCore.OxyCore import com.oxy.AndroidOxyCore.OxyCoreEvent import kotlinx.android.synthetic.main.activity_main.* import java.util.* private lateinit var SDKOxyCore: OxyCore private const val REQUEST_RECORD_AUDIO = 1 private var t = 1 class MainActivity : AppCompatActivity(), OxyCoreEvent { override fun onCreate(savedInstanceState: Bundle?) { super.onCreate(savedInstanceState) setContentView(R.layout.activity_main) //Provide Context for Callback SDKOxyCore = OxyCore(this) } override fun onPause() { super.onPause() //Pause SDK SDKOxyCore.Stop() } override fun onDestroy() { super.onDestroy()c } private fun distance() { SDKOxyCore.distancevol().toString() } //Callback override fun IdWith(Id: String) { when (Id) { \"qa034\" -> { val rnd = Random() val color = Color.argb(255, rnd.nextInt(256), rnd.nextInt(256), rnd.nextInt(256)) mainlayout.setBackgroundColor(color) } else -> { distance() val rnd = Random() Toast.makeText(this@MainActivity, Id, Toast.LENGTH_SHORT).show() val color = Color.argb(255, rnd.nextInt(256), rnd.nextInt(256), rnd.nextInt(256)) mainlayout.setBackgroundColor(color) } } } // Request microphone permissions override fun onResume() { super.onResume() SDKOxyCore.Listen() } }","title":"Android App"},{"location":"tutorials/android-hello/#iphone-app","text":"package com.example.oxysound import android.Manifest import android.content.pm.PackageManager import android.graphics.Color import android.os.Bundle import android.util.Log import android.widget.Toast import androidx.appcompat.app.AppCompatActivity import androidx.core.app.ActivityCompat import com.oxy.AndroidOxyCore.OxyCore import com.oxy.AndroidOxyCore.OxyCoreEvent import kotlinx.android.synthetic.main.activity_main.* import java.util.* private lateinit var SDKOxyCore: OxyCore private const val REQUEST_RECORD_AUDIO = 1 private var t = 1 class MainActivity : AppCompatActivity(), OxyCoreEvent { override fun onCreate(savedInstanceState: Bundle?) { super.onCreate(savedInstanceState) setContentView(R.layout.activity_main) //Provide Context for Callback SDKOxyCore = OxyCore(this) } override fun onPause() { super.onPause() //Pause SDK SDKOxyCore.Stop() } override fun onDestroy() { super.onDestroy()c } private fun distance() { SDKOxyCore.distancevol().toString() } //Callback override fun IdWith(Id: String) { when (Id) { \"qa034\" -> { val rnd = Random() val color = Color.argb(255, rnd.nextInt(256), rnd.nextInt(256), rnd.nextInt(256)) mainlayout.setBackgroundColor(color) } else -> { distance() val rnd = Random() Toast.makeText(this@MainActivity, Id, Toast.LENGTH_SHORT).show() val color = Color.argb(255, rnd.nextInt(256), rnd.nextInt(256), rnd.nextInt(256)) mainlayout.setBackgroundColor(color) } } } // Request microphone permissions override fun onResume() { super.onResume() SDKOxyCore.Listen() } }","title":"iPhone App"},{"location":"tutorials/iphone-hello/","text":"iPhone App \u00b6 import Oxy import UIKit import AVFoundation /** - Note: OxySound Framework Example This class contains common delegate, methods and constants that are used in all application controllers The constructor`OxyManager` contains all the controls of the OxySound SDK **Basic Example SDK configuration:** let OxyManager = Oxy.instance() oxyManager?.delegate=self oxyManager!.listen() Note: The method OxyManager.delegate = self() sets the callback for OK decodes The mehtod OxyManager.instance() initialize the SDK. The method OxyManager.listen() starts the SDK into listening mode the default The method OxyManager.stop() starts the SDK into listening mode the default */ class ViewController: UIViewController, oxyDelegate { @IBOutlet weak var wipbtn: UIButton! //Instance let oxyManager = Oxy.instance() let amax:Float = -110 //door //MARK: Flow control of payload var tog:Bool = false var payload:String = \"\" override func viewDidLoad() { super.viewDidLoad() //Assign delegate oxyManager?.delegate=self //Start engine oxyManager!.listen() } //MARK: Still processing audio push to new thread func oxyId(with oxy_id: String!) { if(self.payload != oxy_id){ self.payload = oxy_id self.tog = true } DispatchQueue.main.async { switch self.payload { case \"91876\": self.view.backgroundColor = self.random() if(self.tog == true) { /* Future use self.tog = false */ self.addimage() } case \"88888\": self.toggleTorch(on: true) self.toggleTorch(on: true) self.toggleTorch(on: false) case \"12345\": //self.view.backgroundColor = self.UIColorFromHex(rgbValue: 0xD61E1E,alpha: 1) //self.view.backgroundColor = self.random() //self.showToast(oxy_id) if(self.oxyManager!.distanceVol() < self.amax ) { print(self.oxyManager!.distanceVol()) self.showToast(\"Move device closer\") self.view.backgroundColor = // Red self.UIColorFromHex(rgbValue: 0xD61E1E,alpha: 1) }else{ self.showToast(\"Device within range\") // Green self.view.backgroundColor = self.UIColorFromHex(rgbValue: 0x228B22,alpha: 1) } default: self.view.backgroundColor = self.random() self.showToast(oxy_id) //print(oxy_id as Any) } } } func random() -> UIColor { return UIColor(red: .random(in: 0...1), green: .random(in: 0...1), blue: .random(in: 0...1), alpha: 1.0) } func addimage(){ var number = Int.random(in: 0 ..< 900) var number2 = Int.random(in: 0 ..< 900) //Create image view simply like this. let imgView = UIImageView() imgView.frame = CGRect(x: number, y: number2, width: 200, height: 200) imgView.image = UIImage(named: \"uv\")//Assign image to ImageView imgView.imgViewCorners() view.addSubview(imgView)//Add image to our view } func UIColorFromHex(rgbValue:UInt32, alpha:Double=1.0)->UIColor { let red = CGFloat((rgbValue & 0xFF0000) >> 16)/256.0 let green = CGFloat((rgbValue & 0xFF00) >> 8)/256.0 let blue = CGFloat(rgbValue & 0xFF)/256.0 return UIColor(red:red, green:green, blue:blue, alpha:CGFloat(alpha)) } func toggleTorch(on: Bool) { guard let device = AVCaptureDevice.default(for: AVMediaType.video), device.hasTorch else { return } do { try device.lockForConfiguration() device.torchMode = on ? .on : .off device.unlockForConfiguration() } catch { print(\"Torch could not be used\") } } } extension UIImageView { //If you want only round corners func imgViewCorners() { layer.cornerRadius = 10 layer.borderWidth = 1.0 layer.masksToBounds = true } }","title":"iPhone App"},{"location":"tutorials/iphone-hello/#iphone-app","text":"import Oxy import UIKit import AVFoundation /** - Note: OxySound Framework Example This class contains common delegate, methods and constants that are used in all application controllers The constructor`OxyManager` contains all the controls of the OxySound SDK **Basic Example SDK configuration:** let OxyManager = Oxy.instance() oxyManager?.delegate=self oxyManager!.listen() Note: The method OxyManager.delegate = self() sets the callback for OK decodes The mehtod OxyManager.instance() initialize the SDK. The method OxyManager.listen() starts the SDK into listening mode the default The method OxyManager.stop() starts the SDK into listening mode the default */ class ViewController: UIViewController, oxyDelegate { @IBOutlet weak var wipbtn: UIButton! //Instance let oxyManager = Oxy.instance() let amax:Float = -110 //door //MARK: Flow control of payload var tog:Bool = false var payload:String = \"\" override func viewDidLoad() { super.viewDidLoad() //Assign delegate oxyManager?.delegate=self //Start engine oxyManager!.listen() } //MARK: Still processing audio push to new thread func oxyId(with oxy_id: String!) { if(self.payload != oxy_id){ self.payload = oxy_id self.tog = true } DispatchQueue.main.async { switch self.payload { case \"91876\": self.view.backgroundColor = self.random() if(self.tog == true) { /* Future use self.tog = false */ self.addimage() } case \"88888\": self.toggleTorch(on: true) self.toggleTorch(on: true) self.toggleTorch(on: false) case \"12345\": //self.view.backgroundColor = self.UIColorFromHex(rgbValue: 0xD61E1E,alpha: 1) //self.view.backgroundColor = self.random() //self.showToast(oxy_id) if(self.oxyManager!.distanceVol() < self.amax ) { print(self.oxyManager!.distanceVol()) self.showToast(\"Move device closer\") self.view.backgroundColor = // Red self.UIColorFromHex(rgbValue: 0xD61E1E,alpha: 1) }else{ self.showToast(\"Device within range\") // Green self.view.backgroundColor = self.UIColorFromHex(rgbValue: 0x228B22,alpha: 1) } default: self.view.backgroundColor = self.random() self.showToast(oxy_id) //print(oxy_id as Any) } } } func random() -> UIColor { return UIColor(red: .random(in: 0...1), green: .random(in: 0...1), blue: .random(in: 0...1), alpha: 1.0) } func addimage(){ var number = Int.random(in: 0 ..< 900) var number2 = Int.random(in: 0 ..< 900) //Create image view simply like this. let imgView = UIImageView() imgView.frame = CGRect(x: number, y: number2, width: 200, height: 200) imgView.image = UIImage(named: \"uv\")//Assign image to ImageView imgView.imgViewCorners() view.addSubview(imgView)//Add image to our view } func UIColorFromHex(rgbValue:UInt32, alpha:Double=1.0)->UIColor { let red = CGFloat((rgbValue & 0xFF0000) >> 16)/256.0 let green = CGFloat((rgbValue & 0xFF00) >> 8)/256.0 let blue = CGFloat(rgbValue & 0xFF)/256.0 return UIColor(red:red, green:green, blue:blue, alpha:CGFloat(alpha)) } func toggleTorch(on: Bool) { guard let device = AVCaptureDevice.default(for: AVMediaType.video), device.hasTorch else { return } do { try device.lockForConfiguration() device.torchMode = on ? .on : .off device.unlockForConfiguration() } catch { print(\"Torch could not be used\") } } } extension UIImageView { //If you want only round corners func imgViewCorners() { layer.cornerRadius = 10 layer.borderWidth = 1.0 layer.masksToBounds = true } }","title":"iPhone App"}]}